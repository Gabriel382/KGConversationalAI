# ğŸ¤ğŸ§ ğŸ”Š Graph-Driven Voice Agent with TAO Cycle

_A modular, fully local conversational AI system following the Thought-Action-Observation cycle._

[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Ollama](https://img.shields.io/badge/LLM-Ollama-green)](https://ollama.com/)
[![Whisper](https://img.shields.io/badge/ASR-Whisper-lightgrey)](https://github.com/openai/whisper)
[![Transformers](https://img.shields.io/badge/NLU-Transformers-blue)](https://huggingface.co/)

---

**Graph-Driven Voice Agent** is a voice-controlled conversational agent that follows the **Thought-Action-Observation (TAO) Cycle**.  
It uses **local speech-to-text**, **intent detection**, **graph-based dialogue management**, **optional FAQ retrieval**, and **LLM-driven response generation** â€” all running **entirely offline**.

Designed to simulate realistic **phone call interactions** through **modular components**.

---

## ğŸ“‚ Project Structure

```
graph_voice_agent/
â”œâ”€â”€ main.py                  # Runs the full TAO cycle
â”œâ”€â”€ observation/              # Captures and processes user speech
â”‚   â””â”€â”€ speech_to_text.py
â”œâ”€â”€ thought/                  # Reasoning, planning, and decision-making
â”‚   â”œâ”€â”€ intent_detector.py
â”‚   â”œâ”€â”€ dialogue_manager.py
â”‚   â”œâ”€â”€ faq_retriever.py
â”‚   â””â”€â”€ response_generator.py
â”œâ”€â”€ action/                   # Acting back into the environment
â”‚   â””â”€â”€ text_to_speech.py
â”œâ”€â”€ dialogue_graph/           # Dialogue and template definitions
â”‚   â”œâ”€â”€ conversation_graph.json
â”‚   â”œâ”€â”€ faq.json
â”‚   â””â”€â”€ templates.json
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ imgs/                     # Project diagrams and images
â”‚   â””â”€â”€ uml.png
â”œâ”€â”€ requirements.txt          # Project dependencies
â”œâ”€â”€ LICENSE                   # Project license
â””â”€â”€ README.md                 # Project documentation
```

---

## ğŸ”¥ Key Features

- **TAO Cycle**: Structured into Observation â†’ Thought â†’ Action phases.
- **Speech-to-Text**: Real-time voice transcription using OpenAI Whisper (locally).
- **Intent Detection**: Zero-shot classification with Hugging Face Transformers.
- **Graph-Based Dialogue Manager**: Control conversation flow via JSON graph.
- **FAQ Retrieval**: Optional, using zero-shot matching over a knowledge base.
- **Natural Language Generation**: Smart responses using Ollama and LLaMA 3 locally.
- **Text-to-Speech**: Voice output via pyttsx3 (offline).
- **Fully Offline Operation**: No paid APIs required.

---

## ğŸ§© Technologies Used

| Category | Technology |
|:---------|:------------|
| ASR (Speech Recognition) | [Whisper](https://github.com/openai/whisper) |
| NLU (Intent Detection) | [Transformers](https://huggingface.co/) |
| NLG (Response Generation) | [Ollama](https://ollama.com/) (LLaMA 3) |
| Graph Traversal | Custom JSON-based dialogue manager |
| TTS (Speech Output) | [pyttsx3](https://pyttsx3.readthedocs.io/en/latest/) |
| Programming Language | Python 3.10+ |

---

## ğŸš€ Getting Started

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Set up Whisper

```bash
pip install openai-whisper
pip install sounddevice numpy scipy
```

### 3. Set up Transformers (Intent Detection)

```bash
pip install transformers
```

### 4. (Optional) Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\activate
```

### 5. Set up Ollama (Local LLM Engine)

- Download and install Ollama from [official website](https://ollama.com/).
- Pull the LLaMA 3 model:
    ```bash
    ollama pull llama3
    ```
- Start the Ollama server:
    ```bash
    ollama serve
    ```

> Ollama API will run locally at `http://localhost:11434`. Keep it running during agent execution.

### 6. Run the Voice Agent

```bash
python main.py
```

âœ… Speak when prompted, and the system will respond intelligently through speech.

---

## ğŸ§  TAO Cycle Architecture Diagram

```mermaid
flowchart TD
    A[Microphone Input] --> B[Observation Phase\nSpeech-to-Text (Whisper)]
    B --> C[Thought Phase\nIntent Detection (Zero-Shot)]
    C --> D[Dialogue Manager\nGraph Traversal]
    D --> E{Needs Knowledge?}
    E -- Yes --> F[FAQ Retriever\nText Matching]
    E -- No --> G
    F --> G[Response Generator\nTemplate + Ollama LLM + History]
    G --> H[Action Phase\nText-to-Speech (pyttsx3)]
    H --> I[Spoken Output]
```

## ğŸ—‚ï¸ Full System UML Diagram

The following UML diagram shows the full architecture, including all modules and internal dependencies:

![UML Diagram](imgs/uml.png)

---

## ğŸŒŸ Future Improvements

- ğŸŒ Add a simple GUI (Streamlit / Gradio)
- ğŸ§  Dynamic dialogue graph updates
- ğŸ—£ï¸ Multi-language support (templates and FAQs)
- ğŸ” Real-time external knowledge fetching
- ğŸ§  Fine-tuning lightweight local LLMs
- ğŸ’¾ Session memory persistence

---

## ğŸ“š License

Distributed under the **MIT License**. See [LICENSE](LICENSE) for more information.

Developed by **Gabriel Henrique Alencar Medeiros**.

---
