# ğŸ¤ğŸ§ ğŸ”Š Graph-Driven Voice Agent with TAO Cycle

*A modular, fully local conversational AI system following the Thought-Action-Observation cycle.*

[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Ollama](https://img.shields.io/badge/LLM-Ollama-green)](https://ollama.com/)
[![Whisper](https://img.shields.io/badge/ASR-Whisper-lightgrey)](https://github.com/openai/whisper)
[![Transformers](https://img.shields.io/badge/NLU-Transformers-blue)](https://huggingface.co/)

---

**Graph-Driven Voice Agent** is a voice-controlled conversational agent that follows the **Thought-Action-Observation (TAO) Cycle**.
It uses **real-time voice detection (VAD)**, **speech-to-text via Whisper**, **zero-shot intent classification**, **graph-based dialogue**, **optional knowledge retrieval**, and **LLM-driven generation** â€” all offline or via local models.

---

## ğŸ“‚ Project Structure

```
graph_voice_agent/
â”œâ”€â”€ main.py                     # Run the TAO loop
â”œâ”€â”€ observation/                # Speech input (with VAD support)
â”‚   â””â”€â”€ speech_to_text.py
â”œâ”€â”€ thought/                    # Reasoning and planning
â”‚   â”œâ”€â”€ intent_detector.py
â”‚   â”œâ”€â”€ dialogue_manager.py
â”‚   â”œâ”€â”€ faq_retriever.py
â”‚   â””â”€â”€ response_generator.py
â”œâ”€â”€ action/                     # Speaking responses
â”‚   â””â”€â”€ text_to_speech.py
â”œâ”€â”€ dialogue_graph/             # Dialogue logic and templates
â”‚   â”œâ”€â”€ conversation_graph.json
â”‚   â”œâ”€â”€ faq.json
â”‚   â””â”€â”€ templates.json
â”œâ”€â”€ services/                   # Encapsulated logic for LLM and DB
â”‚   â”œâ”€â”€ db_faq.py
â”‚   â”œâ”€â”€ llm_classifier.py
â”‚   â””â”€â”€ llm_generator/
â”‚       â”œâ”€â”€ base.py
â”‚       â”œâ”€â”€ api_base.py
â”‚       â”œâ”€â”€ openrouter.py
â”‚       â””â”€â”€ ollama.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ graph_utils.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ imgs/                       # Architecture diagrams
â”‚   â””â”€â”€ uml.png
â”œâ”€â”€ secrets/                    # API key (excluded from Git)
â”‚   â””â”€â”€ apikey.json
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”¥ Key Features

* âœ… **Real-Time VAD (Voice Activity Detection)** using `webrtcvad`
* ğŸ—£ï¸ **Multilingual ASR + English translation** via Whisper
* ğŸ§  **Intent Detection** via Hugging Face zero-shot models
* ğŸ”€ **Graph-Based Dialogue** with modular transitions
* ğŸ“š **FAQ Answering** per dialogue state
* ğŸ§¾ **Template + LLM Response Generation**
* ğŸ”Š **Offline Text-to-Speech (pyttsx3)**
* ğŸ”Œ **Fully offline or API-based setups**

---

## ğŸ§© Technologies Used

| Category         | Technology                                                          |
| ---------------- | ------------------------------------------------------------------- |
| ASR (Speech)     | [Whisper](https://github.com/openai/whisper)                        |
| VAD              | [webrtcvad](https://github.com/wiseman/py-webrtcvad)                |
| Intent Detection | [Transformers](https://huggingface.co/)                             |
| Dialogue Manager | JSON-based state graphs                                             |
| LLM Generator    | [Ollama](https://ollama.com/), [OpenRouter](https://openrouter.ai/) |
| TTS              | [pyttsx3](https://pyttsx3.readthedocs.io/)                          |
| Language         | Python 3.10+                                                        |

---

## ğŸš€ Getting Started

### 1. Install Python packages

```bash
pip install -r requirements.txt
```

If `webrtcvad` fails to install on Windows, install Visual Studio Build Tools:
ğŸ‘‰ [https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

### 2. Install Whisper + Audio dependencies

```bash
pip install openai-whisper sounddevice numpy scipy
```

### 3. (Optional) Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: .\venv\Scripts\activate
```

### 4. Ollama Setup (Local LLMs)

* Install Ollama: [https://ollama.com/](https://ollama.com/)
* Pull a model:

  ```bash
  ollama pull llama3
  ```
* Start the server:

  ```bash
  ollama serve
  ```

### 5. (Optional) Use OpenRouter API

* Create a file at `secrets/apikey.json`:

```json
{
  "api_key": "your-openrouter-key"
}
```

* Run with:

```bash
python main.py --mode api
```

### 6. Run the Voice Agent

```bash
python main.py
```

Speak clearly when prompted. The system will listen, classify your intent, retrieve knowledge, and respond.

---

## ğŸ§  TAO Cycle Architecture

![TAO Architecture](imgs/uml.png)

---

## âœ… Example Intents & States

* `greet â†’ ask_information`
* `book_meeting â†’ confirm_booking â†’ end`
* `cancel â†’ end`
* `ask_information â†’ provide_information`

---

## ğŸŒŸ Coming Soon


* ğŸ“¡ Knowledge-based (Ontology + KG) integration
* ğŸ§  Fine-tuned local intent models
* ğŸ’¬ Session memory and logging
* ğŸŒ Full multilingual response support
* ğŸ›ï¸ GUI (Streamlit or Gradio) ?

---

## ğŸ“š License

MIT License â€” see [LICENSE](LICENSE)

Developed by **Gabriel Henrique Alencar Medeiros**
